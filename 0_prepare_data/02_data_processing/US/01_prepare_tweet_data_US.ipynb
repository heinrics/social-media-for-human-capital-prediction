{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process US tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes and cleans the data, assigns each user/tweets to a county and computes tweet/user indicators\n",
    "\n",
    "CREATES:\n",
    "- ID file for all included tweets/users with link to county of tweet and user\n",
    "- Tweet level data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakob\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Import modules\n",
    "################################################################################\n",
    "import os\n",
    "import dask\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "import swifter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set utilities\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Specify paths\n",
    "################################################################################\n",
    "os.chdir(\"../..\")\n",
    "mode = \"full\" # sample\n",
    "if mode == \"full\":\n",
    "    input_path = \"PATH TO FOLDER\"\n",
    "    output_path = \"PATH TO FOLDER\"\n",
    "if mode == \"sample\":\n",
    "    input_path = \"PATH TO FOLDER\"\n",
    "    output_path = \"PATH TO FOLDER\"\n",
    "geo_path = \"PATH TO FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets for 1489 hours = 62.0 days = 2.1 months!\n"
     ]
    }
   ],
   "source": [
    "hours = len(glob.glob(input_path))\n",
    "print(f\"Tweets for {hours} hours = {round(hours/24, 1)} days = {round(hours/24/30,1)} months!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tweets = db.read_text(input_path) \\\n",
    "            .filter(lambda x: x != '\\n') \\\n",
    "            .filter(lambda x: \"created_at\" in x) \\\n",
    "            .map(lambda x: x.replace(\"\\n\", \"\")) \\\n",
    "            .map(json.loads)\n",
    "\n",
    "tweet_info = tweets.map(lambda x: [x[\"id_str\"],\n",
    "                                   x[\"created_at\"],\n",
    "                                   x[\"extended_tweet\"][\"full_text\"] if \"extended_tweet\" in x else x[\"text\"],\n",
    "                                   x[\"user\"][\"id_str\"],\n",
    "                                   x[\"user\"][\"followers_count\"],\n",
    "                                   x[\"user\"][\"friends_count\"],\n",
    "                                   x[\"user\"][\"statuses_count\"],\n",
    "                                   x[\"user\"][\"verified\"],\n",
    "                                   x[\"user\"][\"description\"],\n",
    "                                   x[\"user\"][\"created_at\"],\n",
    "                                   x[\"place\"][\"country_code\"] if x[\"place\"]!= None else None,\n",
    "                                   x[\"coordinates\"][\"coordinates\"] if x[\"coordinates\"] != None else None,\n",
    "                                   x[\"place\"][\"place_type\"] if x[\"place\"] != None else None,\n",
    "                                   x[\"place\"][\"name\"] if x[\"place\"] != None else None,\n",
    "                                   x[\"place\"][\"bounding_box\"][\"coordinates\"][0] if x[\"place\"] != None else None,\n",
    "                                   x[\"is_quote_status\"],\n",
    "                                   x[\"user\"][\"is_translator\"],\n",
    "                                   x[\"source\"],\n",
    "                                   x[\"lang\"]\n",
    "                                  ])\n",
    "\n",
    "\n",
    "tweet_dd = tweet_info.to_dataframe(columns = [\"id\",\n",
    "                                              \"created_at\",\n",
    "                                              \"tweet\",\n",
    "                                              \"user_id\",\n",
    "                                              \"user_followers\",\n",
    "                                              \"user_friends\",\n",
    "                                              \"user_statuses\",\n",
    "                                              \"user_verified\",\n",
    "                                              \"user_description\",\n",
    "                                              \"user_created_at\",\n",
    "                                              \"country_code\",\n",
    "                                              \"coordinates\",\n",
    "                                              \"place_type\",\n",
    "                                              \"place_name\",\n",
    "                                              \"bounding_box\",\n",
    "                                              \"is_quote_status\",\n",
    "                                              \"is_translator\",\n",
    "                                              \"source\",\n",
    "                                              \"language\"\n",
    "                                             ])\n",
    "\n",
    "\n",
    "\n",
    "# Keep only English tweets in the US (and drop translators)\n",
    "tweet_dd = tweet_dd[(tweet_dd[\"country_code\"] == \"US\") & (tweet_dd[\"language\"] == \"en\")]\n",
    "tweet_dd = tweet_dd[tweet_dd[\"is_translator\"]==False]\n",
    "\n",
    "tweet_dd = tweet_dd.drop([\"is_translator\", \"country_code\", \"language\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Dask DataFrame Structure:\n                      id created_at   tweet user_id user_followers user_friends user_statuses user_verified user_description user_created_at coordinates place_type place_name bounding_box is_quote_status  source\nnpartitions=1489                                                                                                                                                                                                   \n                  object     object  object  object          int64        int64         int64          bool           object          object      object     object     object       object            bool  object\n                     ...        ...     ...     ...            ...          ...           ...           ...              ...             ...         ...        ...        ...          ...             ...     ...\n...                  ...        ...     ...     ...            ...          ...           ...           ...              ...             ...         ...        ...        ...          ...             ...     ...\n                     ...        ...     ...     ...            ...          ...           ...           ...              ...             ...         ...        ...        ...          ...             ...     ...\n                     ...        ...     ...     ...            ...          ...           ...           ...              ...             ...         ...        ...        ...          ...             ...     ...\nDask Name: drop_by_shallow_copy, 11 graph layers",
      "text/html": "<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>created_at</th>\n      <th>tweet</th>\n      <th>user_id</th>\n      <th>user_followers</th>\n      <th>user_friends</th>\n      <th>user_statuses</th>\n      <th>user_verified</th>\n      <th>user_description</th>\n      <th>user_created_at</th>\n      <th>coordinates</th>\n      <th>place_type</th>\n      <th>place_name</th>\n      <th>bounding_box</th>\n      <th>is_quote_status</th>\n      <th>source</th>\n    </tr>\n    <tr>\n      <th>npartitions=1489</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th></th>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>int64</td>\n      <td>int64</td>\n      <td>int64</td>\n      <td>bool</td>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>object</td>\n      <td>bool</td>\n      <td>object</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div>Dask Name: drop_by_shallow_copy, 11 graph layers</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Inspect, clean and prepare georeferenced data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Clean and inspect geodata of tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city            24005723\n",
      "admin            4669357\n",
      "poi               538962\n",
      "neighborhood       49219\n",
      "country              237\n",
      "Name: place_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "place_types = tweet_dd.place_type.value_counts().compute(scheduler='processes')\n",
    "print(place_types)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                     id                      created_at  \\\n5   1421591480309747719  Sat Jul 31 21:59:55 +0000 2021   \n18  1421591483174510596  Sat Jul 31 21:59:56 +0000 2021   \n26  1421591485737222153  Sat Jul 31 21:59:56 +0000 2021   \n59  1421591497028288514  Sat Jul 31 21:59:59 +0000 2021   \n74  1421591500958441475  Sat Jul 31 22:00:00 +0000 2021   \n\n                                                                                                  tweet  \\\n5   See our latest #Rochester, NY #Engineering job opportunity and click the link in our bio to appl...   \n18                          Just posted a photo @ Mt. Mitchell - Elevation 6684 https://t.co/34rJFZBJOf   \n26  We made it. #Blippiliveshow #Blippi #soexcited #Kadesfirstshow @ York State Fair https://t.co/GN...   \n59                                                                  @richardaeden Shame on all of them.   \n74                                    Summer in repose. @ Gilgo Beach, New York https://t.co/NAbD0KLQin   \n\n       user_id  user_followers  user_friends  user_statuses  user_verified  \\\n5     24634756             204           162            514          False   \n18  2482948820             401          1856          46410          False   \n26   236684450              43            81           5072          False   \n59  4893882268             105           295           4225          False   \n74    24615661             162           102          11827          False   \n\n                                                                                       user_description  \\\n5   Follow this account for geo-targeted Engineering job tweets in Rochester, NY. Need help? Tweet u...   \n18  #blacklivesmatter Haudenosaunee/Seneca/VETERAN/Plant-eater/Indiqueer👬🏳️‍🌈/Sober(11)/(+/u)/Home: ...   \n26                                                                              Curiouser and Curiouser   \n59                                                                                                 None   \n74                                         graphic designer. king of the kooks. just trying to survive.   \n\n                   user_created_at                  coordinates place_type  \\\n5   Mon Mar 16 02:31:01 +0000 2009      [-77.6109219, 43.16103]      admin   \n18  Thu May 08 00:28:48 +0000 2014  [-82.26511002, 35.76587566]      admin   \n26  Tue Jan 11 04:10:50 +0000 2011   [-76.75563812, 39.9574514]      admin   \n59  Wed Feb 10 02:22:48 +0000 2016                         None      admin   \n74  Mon Mar 16 00:22:47 +0000 2009          [-73.3981, 40.6183]      admin   \n\n        place_name  \\\n5         New York   \n18  North Carolina   \n26    Pennsylvania   \n59    Pennsylvania   \n74        New York   \n\n                                                                                           bounding_box  \\\n5    [[-79.76259, 40.477383], [-79.76259, 45.015851], [-71.777492, 45.015851], [-71.777492, 40.477383]]   \n18   [[-84.321948, 33.752879], [-84.321948, 36.588118], [-75.40012, 36.588118], [-75.40012, 33.752879]]   \n26  [[-80.519851, 39.719801], [-80.519851, 42.516072], [-74.689517, 42.516072], [-74.689517, 39.7198...   \n59  [[-80.519851, 39.719801], [-80.519851, 42.516072], [-74.689517, 42.516072], [-74.689517, 39.7198...   \n74   [[-79.76259, 40.477383], [-79.76259, 45.015851], [-71.777492, 45.015851], [-71.777492, 40.477383]]   \n\n    is_quote_status  \\\n5             False   \n18            False   \n26            False   \n59            False   \n74            False   \n\n                                                                                source  \n5                 <a href=\"https://www.careerarc.com\" rel=\"nofollow\">CareerArc 2.0</a>  \n18                         <a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>  \n26                         <a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>  \n59  <a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>  \n74                         <a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>created_at</th>\n      <th>tweet</th>\n      <th>user_id</th>\n      <th>user_followers</th>\n      <th>user_friends</th>\n      <th>user_statuses</th>\n      <th>user_verified</th>\n      <th>user_description</th>\n      <th>user_created_at</th>\n      <th>coordinates</th>\n      <th>place_type</th>\n      <th>place_name</th>\n      <th>bounding_box</th>\n      <th>is_quote_status</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>1421591480309747719</td>\n      <td>Sat Jul 31 21:59:55 +0000 2021</td>\n      <td>See our latest #Rochester, NY #Engineering job opportunity and click the link in our bio to appl...</td>\n      <td>24634756</td>\n      <td>204</td>\n      <td>162</td>\n      <td>514</td>\n      <td>False</td>\n      <td>Follow this account for geo-targeted Engineering job tweets in Rochester, NY. Need help? Tweet u...</td>\n      <td>Mon Mar 16 02:31:01 +0000 2009</td>\n      <td>[-77.6109219, 43.16103]</td>\n      <td>admin</td>\n      <td>New York</td>\n      <td>[[-79.76259, 40.477383], [-79.76259, 45.015851], [-71.777492, 45.015851], [-71.777492, 40.477383]]</td>\n      <td>False</td>\n      <td>&lt;a href=\"https://www.careerarc.com\" rel=\"nofollow\"&gt;CareerArc 2.0&lt;/a&gt;</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1421591483174510596</td>\n      <td>Sat Jul 31 21:59:56 +0000 2021</td>\n      <td>Just posted a photo @ Mt. Mitchell - Elevation 6684 https://t.co/34rJFZBJOf</td>\n      <td>2482948820</td>\n      <td>401</td>\n      <td>1856</td>\n      <td>46410</td>\n      <td>False</td>\n      <td>#blacklivesmatter Haudenosaunee/Seneca/VETERAN/Plant-eater/Indiqueer👬🏳️‍🌈/Sober(11)/(+/u)/Home: ...</td>\n      <td>Thu May 08 00:28:48 +0000 2014</td>\n      <td>[-82.26511002, 35.76587566]</td>\n      <td>admin</td>\n      <td>North Carolina</td>\n      <td>[[-84.321948, 33.752879], [-84.321948, 36.588118], [-75.40012, 36.588118], [-75.40012, 33.752879]]</td>\n      <td>False</td>\n      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;Instagram&lt;/a&gt;</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1421591485737222153</td>\n      <td>Sat Jul 31 21:59:56 +0000 2021</td>\n      <td>We made it. #Blippiliveshow #Blippi #soexcited #Kadesfirstshow @ York State Fair https://t.co/GN...</td>\n      <td>236684450</td>\n      <td>43</td>\n      <td>81</td>\n      <td>5072</td>\n      <td>False</td>\n      <td>Curiouser and Curiouser</td>\n      <td>Tue Jan 11 04:10:50 +0000 2011</td>\n      <td>[-76.75563812, 39.9574514]</td>\n      <td>admin</td>\n      <td>Pennsylvania</td>\n      <td>[[-80.519851, 39.719801], [-80.519851, 42.516072], [-74.689517, 42.516072], [-74.689517, 39.7198...</td>\n      <td>False</td>\n      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;Instagram&lt;/a&gt;</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>1421591497028288514</td>\n      <td>Sat Jul 31 21:59:59 +0000 2021</td>\n      <td>@richardaeden Shame on all of them.</td>\n      <td>4893882268</td>\n      <td>105</td>\n      <td>295</td>\n      <td>4225</td>\n      <td>False</td>\n      <td>None</td>\n      <td>Wed Feb 10 02:22:48 +0000 2016</td>\n      <td>None</td>\n      <td>admin</td>\n      <td>Pennsylvania</td>\n      <td>[[-80.519851, 39.719801], [-80.519851, 42.516072], [-74.689517, 42.516072], [-74.689517, 39.7198...</td>\n      <td>False</td>\n      <td>&lt;a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\"&gt;Twitter for iPhone&lt;/a&gt;</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>1421591500958441475</td>\n      <td>Sat Jul 31 22:00:00 +0000 2021</td>\n      <td>Summer in repose. @ Gilgo Beach, New York https://t.co/NAbD0KLQin</td>\n      <td>24615661</td>\n      <td>162</td>\n      <td>102</td>\n      <td>11827</td>\n      <td>False</td>\n      <td>graphic designer. king of the kooks. just trying to survive.</td>\n      <td>Mon Mar 16 00:22:47 +0000 2009</td>\n      <td>[-73.3981, 40.6183]</td>\n      <td>admin</td>\n      <td>New York</td>\n      <td>[[-79.76259, 40.477383], [-79.76259, 45.015851], [-71.777492, 45.015851], [-71.777492, 40.477383]]</td>\n      <td>False</td>\n      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;Instagram&lt;/a&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if admin is state or county\n",
    "tweet_dd[tweet_dd.place_type == \"admin\"].head() # = state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "id                  25072902\ncreated_at          25072902\ntweet               25072902\nuser_id             25072902\nuser_followers      25072902\nuser_friends        25072902\nuser_statuses       25072902\nuser_verified       25072902\nuser_description    23308464\nuser_created_at     25072902\ncoordinates          2976632\nplace_type          25072902\nplace_name          25072902\nbounding_box        25072902\nis_quote_status     25072902\nsource              25072902\ndtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop country and state level data\n",
    "tweet_dd = tweet_dd[(tweet_dd[\"place_type\"].isin([\"country\", \"admin\"]) == False) | (tweet_dd[\"coordinates\"]).notnull()]\n",
    "tweet_dd.count().compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Convert tweet data to geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city            21522995\n",
      "exact            2976632\n",
      "poi               538687\n",
      "neighborhood       34588\n",
      "Name: place_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check and clean place types\n",
    "df = tweet_dd.compute(scheduler='processes')\n",
    "df.loc[df[\"coordinates\"].notnull(), \"place_type\"] = \"exact\"\n",
    "df.loc[df[\"coordinates\"].notnull(), \"place_name\"] = np.nan\n",
    "print(df.place_type.value_counts())\n",
    "df.place_type.isna().sum() # Should be 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0    POLYGON ((-115.384091 36.129459, -115.384091 36.336371, -115.062159 36.336371, -115.062159 36.12...\n4    POLYGON ((-74.60247 40.37991, -74.60247 40.40879, -74.548858 40.40879, -74.548858 40.37991, -74....\n5    POLYGON ((-79.76259 40.477383, -79.76259 45.015851, -71.777492 45.015851, -71.777492 40.477383, ...\n7    POLYGON ((-75.742439 39.409781, -75.742439 39.477614, -75.665128 39.477614, -75.665128 39.409781...\n8    POLYGON ((-112.323914 33.29026, -112.323914 33.815465, -111.925439 33.815465, -111.925439 33.290...\nName: bounding_box, dtype: object"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert bounding box and exact coordinates to geometry\n",
    "df[\"bounding_box\"] = df[\"bounding_box\"].apply(Polygon)\n",
    "df[\"coordinates\"] = df[\"coordinates\"].apply(lambda x: Point(x) if x != None else np.nan)\n",
    "df[\"bounding_box\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0    POINT (-115.22312499999998 36.232915)\n4              POINT (-74.575664 40.39435)\n5             POINT (-77.6109219 43.16103)\n7    POINT (-75.70378350000001 39.4436975)\n8          POINT (-112.1246765 33.5528625)\nName: best_coords, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute best coordinates (exact coords if available, else centroid of bounding box)\n",
    "df[\"best_coords\"] = df.apply(lambda x: x[\"coordinates\"] if pd.notna(x[\"coordinates\"])\n",
    "                                     else x[\"bounding_box\"].centroid, axis=1)\n",
    "df.drop(\"coordinates\", inplace = True, axis = 1)\n",
    "df[\"best_coords\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df.loc[df[\"place_type\"] == \"exact\", \"bounding_box\"] = np.nan # Set bounding box to na if exact coordinates are available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    POINT (-115.22312499999998 36.232915)\n",
      "4              POINT (-74.575664 40.39435)\n",
      "Name: best_coords, dtype: object\n",
      "0    POLYGON ((-1716840.273 1628937.801, -1712146.067 1651631.754, -1684096.318 1645879.236, -1688713...\n",
      "4    POLYGON ((1785952.173 2131454.925, 1785229.878 2134608.471, 1789626.683 2135616.829, 1790350.756...\n",
      "Name: bounding_box, dtype: geometry\n"
     ]
    }
   ],
   "source": [
    "# Change projection (to make map look better)\n",
    "df = df.set_geometry('bounding_box')\n",
    "df.crs = {'init': 'epsg:4326', 'no_defs': True}\n",
    "df = df.to_crs(\"EPSG:5071\")\n",
    "for column in [\"best_coords\", \"bounding_box\"]:\n",
    "    print(df[column].head(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    POINT (-1700451.589 1637392.290)\n",
      "4     POINT (1787789.942 2133535.676)\n",
      "Name: best_coords, dtype: geometry\n",
      "0    POLYGON ((-1716840.273 1628937.801, -1712146.067 1651631.754, -1684096.318 1645879.236, -1688713...\n",
      "4    POLYGON ((1785952.173 2131454.925, 1785229.878 2134608.471, 1789626.683 2135616.829, 1790350.756...\n",
      "Name: bounding_box, dtype: geometry\n"
     ]
    }
   ],
   "source": [
    "df = df.set_geometry('best_coords')\n",
    "df.crs = {'init': 'epsg:4326', 'no_defs': True}\n",
    "df = df.to_crs(\"EPSG:5071\")\n",
    "for column in [\"best_coords\", \"bounding_box\"]:\n",
    "    print(df[column].head(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsg:4269\n",
      "EPSG:5071\n"
     ]
    }
   ],
   "source": [
    "# Import shp-file on us counties and change projection\n",
    "counties = geopandas.read_file(geo_path + r\"\\cb_2018_us_county_500k.shp\") #!!!!!!!!! CHANGED SHP-file\n",
    "print(counties.crs)\n",
    "counties = counties.to_crs(\"EPSG:5071\")\n",
    "print(counties.crs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "data": {
      "text/plain": "              NAME  GEOID\n26  Aleutians West  02016\n28            Nome  02180\n29         Yakutat  02282\n46          Nevada  06057\n73          Fulton  13121",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>GEOID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>Aleutians West</td>\n      <td>02016</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Nome</td>\n      <td>02180</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Yakutat</td>\n      <td>02282</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Nevada</td>\n      <td>06057</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Fulton</td>\n      <td>13121</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mark counties where centroid lies in other county\n",
    "counties[\"centroid_bb\"] = counties[\"geometry\"].envelope.centroid\n",
    "counties[\"centroid_in_mun\"] = counties[\"geometry\"].contains(counties[\"centroid_bb\"] )\n",
    "counties[\"centroid_in_mun\"].value_counts()\n",
    "print(len(counties.loc[counties[\"centroid_in_mun\"] == False, [\"NAME\", \"GEOID\"]]))\n",
    "counties.loc[counties[\"centroid_in_mun\"] == False, [\"NAME\", \"GEOID\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     1478\n",
      "2      470\n",
      "3      261\n",
      "4      108\n",
      "5      105\n",
      "12      96\n",
      "8       96\n",
      "6       84\n",
      "11      66\n",
      "9       63\n",
      "10      60\n",
      "18      54\n",
      "26      52\n",
      "24      48\n",
      "7       35\n",
      "17      34\n",
      "31      31\n",
      "14      28\n",
      "20      20\n",
      "16      16\n",
      "15      15\n",
      "13      13\n",
      "Name: NAME, dtype: int64\n",
      "1    3221\n",
      "2      12\n",
      "Name: NAME, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check numer of counties with non-unique names (same names in different states)\n",
    "print(counties.groupby(\"NAME\")[\"NAME\"].transform(\"count\").value_counts())\n",
    "print(counties.groupby([\"STATEFP\", \"NAME\"])[\"NAME\"].transform(\"count\").value_counts()) # Almost unique within states (6 problem counties)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Join with counties dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Note: First performs different types of joins and then compares and checks consistency to perform final join"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "count    2.507290e+07\n",
      "mean     5.880555e+02\n",
      "std      1.041037e+03\n",
      "min      0.000000e+00\n",
      "25%      3.264912e+01\n",
      "50%      1.552927e+02\n",
      "75%      6.330105e+02\n",
      "max      5.104632e+03\n",
      "Name: bounding_box_area, dtype: float64\n",
      "Counties:\n",
      "count    3.233000e+03\n",
      "mean     6.345393e+03\n",
      "std      4.971142e+04\n",
      "min      5.092680e-01\n",
      "25%      1.583265e+03\n",
      "50%      2.350249e+03\n",
      "75%      3.719008e+03\n",
      "max      2.031607e+06\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute area of bounding box for tweets\n",
    "print(\"Tweets:\")\n",
    "df[\"bounding_box_area\"] = df[\"bounding_box\"].area/(1000*1000)\n",
    "df.loc[df[\"place_type\"] == \"exact\", \"bounding_box_area\"] = 0\n",
    "print(df[\"bounding_box_area\"].describe())\n",
    "\n",
    "# Compute area of bounding box for counties\n",
    "print(\"Counties:\")\n",
    "print((counties[\"geometry\"].envelope.area/(1000*1000)).describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Los Angeles    867372\nHouston        600312\nSan Antonio    231747\nPhoenix        182552\nFort Worth     103604\nCastro             61\nName: place_name, dtype: int64"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"bounding_box_area\"] > 2000, \"place_name\"].value_counts() # Bounding boxes seem to be reasonably precise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 1. Join if tweet bounding box is within county polygon (works if county level precision)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Perform within join for polygons\n",
    "counties.rename({\"geometry\": \"mun_polygon\"}, inplace=True, axis=1)\n",
    "counties = counties.set_geometry('mun_polygon')\n",
    "df = df.set_geometry('bounding_box')\n",
    "df = geopandas.sjoin(df, counties[[\"GEOID\", \"NAME\", \"STATEFP\", \"mun_polygon\"]],\n",
    "                     how='left', op='within')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(\"index_right\", inplace=True, axis=1)\n",
    "\n",
    "# Rename columns\n",
    "df.rename({\"GEOID\": \"mun_id_within\",\n",
    "           \"NAME\": \"mun_name_within\",\n",
    "           \"STATEFP\": \"state_id_within\",\n",
    "           'centroid_in_mun': \"centroid_in_mun_within\"}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    18097506\n",
      "True      6975396\n",
      "Name: mun_id_within, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "place_type  place_name \ncity        Los Angeles    867372\n            Houston        600312\n            Chicago        513185\n            Manhattan      447056\n            Brooklyn       367584\ndtype: int64"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "print(df[\"mun_id_within\"].notna().value_counts()) # Only about 1/4 of observations matched\n",
    "df.loc[df[\"mun_id_within\"].isna(), [\"place_type\", \"place_name\"]].value_counts().head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 2. Join if tweet bounding box contains county polygon (works if only county level precision)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25072902\n",
      "25072902\n"
     ]
    }
   ],
   "source": [
    "# Perform contains join for polygons\n",
    "print(len(df))\n",
    "df = geopandas.sjoin(df, counties[[\"GEOID\", \"NAME\", \"STATEFP\", \"mun_polygon\"]],\n",
    "                     how='left', op='contains')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(\"index_right\", inplace=True, axis=1)\n",
    "print(len(df)) # Check if it remains the same (if not, some tweet bounding boxes contain several counties)\n",
    "\n",
    "# Rename columns\n",
    "df.rename({\"GEOID\": \"mun_id_contains\",\n",
    "           \"NAME\": \"mun_name_contains\",\n",
    "           \"STATEFP\": \"state_id_contains\",}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    25061848\n",
      "True        11054\n",
      "Name: mun_id_contains, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check nr of tweets that were merged\n",
    "print(df[\"mun_id_contains\"].notna().value_counts()) # Very few matches (tweet bounding boxes are not usually county level precision)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# 3. Join with county that contains centroid of bounding box (problem if mun level precision and centroid not in mun)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25072902\n"
     ]
    }
   ],
   "source": [
    "# Perform within join for centroids\n",
    "print(len(df))\n",
    "df = df.set_geometry(\"best_coords\")\n",
    "df = geopandas.sjoin(df, counties[[\"GEOID\", \"NAME\", \"STATEFP\", \"centroid_in_mun\", \"mun_polygon\"]],\n",
    "                     how='left', op='within')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(\"index_right\", inplace=True, axis=1)\n",
    "len(df)\n",
    "\n",
    "# Rename columns\n",
    "df.rename({\"GEOID\": \"mun_id_cwithin\",\n",
    "           \"NAME\": \"mun_name_cwithin\",\n",
    "           \"STATEFP\": \"state_id_cwithin\",\n",
    "           'centroid_in_mun': \"centroid_in_mun_cwithin\"}, axis=1, inplace=True)\n",
    "df.geometry.name = \"best_coords\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     25027902\n",
      "False       45000\n",
      "Name: mun_id_cwithin, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "place_type  place_name  \ncity        Alameda         5394\n            Coronado        3260\n            Provincetown    3190\n            Malibu          2746\n            Key Largo       1739\ndtype: int64"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check merge\n",
    "print(df[\"mun_id_cwithin\"].notna().value_counts()) # Almost all tweets merged\n",
    "df.loc[df[\"mun_id_cwithin\"].isna(), [\"place_type\", \"place_name\"]].value_counts().head(5) # Mostly coastal places"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25072902\n",
      "25072902\n"
     ]
    }
   ],
   "source": [
    "# 4. Join with county with nearest bounding box centroid (only correct if county level precision)\n",
    "counties = counties.set_geometry('centroid_bb')\n",
    "df = df.set_geometry(\"best_coords\")\n",
    "print(len(df))\n",
    "df = geopandas.sjoin_nearest(df, counties[[\"GEOID\",\n",
    "                                       \"NAME\",\n",
    "                                       \"STATEFP\",\n",
    "                                       \"centroid_in_mun\",\n",
    "                                       \"centroid_bb\"]].set_geometry('centroid_bb'))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "# Rename columns\n",
    "df.rename({\"GEOID\": \"mun_id_nearest\",\n",
    "           \"NAME\": \"mun_name_nearest\",\n",
    "           \"STATEFP\": \"state_id_nearest\",\n",
    "           'centroid_in_mun': \"centroid_in_mun_nearest\"}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True    25072902\n",
      "Name: mun_id_nearest, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check merge\n",
    "print(df[\"mun_id_nearest\"].notna().value_counts()) # Everything merged (by design)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check consistency and perform final merge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Note: For matches that are deemed correct, the respective county IDs are inserted into column GEOID"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "city            21522995\nexact            2976632\npoi               538687\nneighborhood       34588\nName: place_type, dtype: int64"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.place_type.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "False    15112131\nTrue      9960771\nName: GEOID, dtype: int64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take (1) within, (2) contains matches for polygons and (3) matches if exact/neighborhood/poi coordinates (safe matches)\n",
    "df[\"GEOID\"] = df.apply(lambda x:\n",
    "                       x[\"mun_id_within\"] if pd.notna(x[\"mun_id_within\"])\n",
    "                       else (x[\"mun_id_contains\"] if pd.notna(x[\"mun_id_contains\"])\n",
    "                       else (x[\"mun_id_cwithin\"] if x[\"place_type\"] in [\"exact\", \"poi\", \"neighborhood\"]\n",
    "                             else np.nan)),\n",
    "                       axis=1)\n",
    "df[\"GEOID\"].notna().value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     23715267\n",
      "False     1312635\n",
      "Name: centroid_in_mun_cwithin, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "True     23993451\nFalse     1079451\nName: GEOID, dtype: int64"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cwithin matches (bb centroids within county) if centroid of matched county is within county\n",
    "print(df[\"centroid_in_mun_cwithin\"].value_counts())\n",
    "df[\"GEOID\"] = df.apply(lambda x:\n",
    "                       x[\"GEOID\"] if pd.notna(x[\"GEOID\"])\n",
    "                       else (x[\"mun_id_cwithin\"] if x[\"centroid_in_mun_cwithin\"] == True\n",
    "                             else np.nan),\n",
    "                       axis=1)\n",
    "df[\"GEOID\"].notna().value_counts() # Very large share of tweets matched"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     21497911\n",
      "False     3574991\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "True     24868523\nFalse      204379\nName: GEOID, dtype: int64"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cwithin matches if cwithin and nearest yield same county\n",
    "print((df[\"mun_id_cwithin\"] == df[\"mun_id_nearest\"]).value_counts())\n",
    "df[\"GEOID\"] = df.apply(lambda x:\n",
    "                       x[\"GEOID\"] if pd.notna(x[\"GEOID\"])\n",
    "                       else (x[\"mun_id_cwithin\"] if x[\"mun_id_cwithin\"] == x[\"mun_id_nearest\"]\n",
    "                             else np.nan),\n",
    "                       axis=1)\n",
    "df[\"GEOID\"].notna().value_counts() # Almost all tweets matched"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "True     24911833\nFalse      161069\nName: GEOID, dtype: int64"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cwithin  if place_names is equal to county name for (1) cwithin and (2) nearest if equal to nearest match\n",
    "df[\"GEOID\"] = df.apply(lambda x:\n",
    "                       x[\"GEOID\"] if pd.notna(x[\"GEOID\"])\n",
    "                       else (x[\"mun_id_cwithin\"] if x[\"mun_name_cwithin\"] == x[\"place_name\"]\n",
    "                       else (x[\"mun_id_nearest\"] if x[\"mun_name_nearest\"] == x[\"place_name\"]\n",
    "                             else np.nan)),\n",
    "                       axis=1)\n",
    "df[\"GEOID\"].notna().value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "True     25028053\nFalse       44849\nName: GEOID, dtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cwithin if bounding box area is small\n",
    "df[\"GEOID\"] = df.apply(lambda x:\n",
    "                       x[\"GEOID\"] if pd.notna(x[\"GEOID\"])\n",
    "                       else (x[\"mun_id_cwithin\"] if x[\"bounding_box_area\"]<300\n",
    "                             else np.nan),\n",
    "                       axis=1)\n",
    "df[\"GEOID\"].notna().value_counts() # Almost all tweets matched"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    44849\n",
      "Name: mun_name_cwithin, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "Series([], dtype: int64)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect remaining tweets\n",
    "print(df[df[\"GEOID\"].isna()][\"mun_name_cwithin\"].notna().value_counts())\n",
    "df.loc[df[\"GEOID\"].isna() & df[\"mun_name_cwithin\"].notna(), [\"place_name\", \"place_type\",\"mun_name_cwithin\", \"mun_name_nearest\"]].value_counts(dropna=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "count    44849.000000\nmean        61.598896\nstd         76.461023\nmin          0.000000\n25%          0.000000\n50%         25.495577\n75%         82.327763\nmax        416.040904\nName: bounding_box_area, dtype: float64"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"GEOID\"].isna()][\"bounding_box_area\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1788743879747147 percent of tweets will be dropped\n"
     ]
    }
   ],
   "source": [
    "# Drop if not assigned to any county\n",
    "print(f\"{df['GEOID'].isna().mean()*100} percent of tweets will be dropped\") # 0.2 % of tweets dropped\n",
    "df = df[df['GEOID'].notna()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Add name and drop redundant variables\n",
    "df = df.merge(counties[[\"NAME\", \"GEOID\"]],  how=\"left\", on = \"GEOID\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "df.drop(['mun_id_within', 'mun_name_within', 'state_id_within', 'mun_id_contains',\n",
    "        'mun_name_contains', 'state_id_contains', 'mun_id_cwithin',\n",
    "        'mun_name_cwithin', 'state_id_cwithin', 'centroid_in_mun_cwithin',\n",
    "        'index_right', 'mun_id_nearest', 'mun_name_nearest', 'state_id_nearest',\n",
    "        'centroid_in_mun_nearest','bounding_box_area'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['id', 'created_at', 'tweet', 'user_id', 'user_followers',\n       'user_friends', 'user_statuses', 'user_verified', 'user_description',\n       'user_created_at', 'place_type', 'place_name', 'bounding_box',\n       'is_quote_status', 'source', 'best_coords', 'GEOID', 'NAME'],\n      dtype='object')"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11057\n"
     ]
    }
   ],
   "source": [
    "# Inspect duplicates\n",
    "df[\"dupl_id\"] = df.duplicated(subset='id', keep = False)\n",
    "print(df.dupl_id.sum())\n",
    "dupls = df[df[\"dupl_id\"]].sort_values(by=[\"user_id\", \"id\"])\n",
    "#dupls.head(200).to_excel(output_path + r\"\\inspect\\check_dupls.xlsx\") # Seem to be real duplicates, not just duplicated ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25028053\n",
      "25021897\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "df.drop([\"dupl_id\"], axis = 1, inplace =True )\n",
    "print(len(df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Exlude bots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Inspect source of tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'H:\\analysis_mapping_mistakes\\US\\01-processed-data\\01-tweet_data\\inspect'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [49], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43minspect\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43msources.xlsx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df\u001B[38;5;241m.\u001B[39msource\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\core\\generic.py:2374\u001B[0m, in \u001B[0;36mNDFrame.to_excel\u001B[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001B[0m\n\u001B[0;32m   2361\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mformats\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexcel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExcelFormatter\n\u001B[0;32m   2363\u001B[0m formatter \u001B[38;5;241m=\u001B[39m ExcelFormatter(\n\u001B[0;32m   2364\u001B[0m     df,\n\u001B[0;32m   2365\u001B[0m     na_rep\u001B[38;5;241m=\u001B[39mna_rep,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2372\u001B[0m     inf_rep\u001B[38;5;241m=\u001B[39minf_rep,\n\u001B[0;32m   2373\u001B[0m )\n\u001B[1;32m-> 2374\u001B[0m \u001B[43mformatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2375\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcel_writer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2376\u001B[0m \u001B[43m    \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msheet_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstartrow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstartrow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2378\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstartcol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstartcol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2379\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfreeze_panes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreeze_panes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2382\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\io\\formats\\excel.py:910\u001B[0m, in \u001B[0;36mExcelFormatter.write\u001B[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001B[0m\n\u001B[0;32m    906\u001B[0m     need_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    907\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001B[39;00m\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001B[39;00m\n\u001B[1;32m--> 910\u001B[0m     writer \u001B[38;5;241m=\u001B[39m \u001B[43mExcelWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[abstract]\u001B[39;49;00m\n\u001B[0;32m    911\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    913\u001B[0m     need_save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:60\u001B[0m, in \u001B[0;36mOpenpyxlWriter.__init__\u001B[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworkbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Workbook\n\u001B[0;32m     58\u001B[0m engine_kwargs \u001B[38;5;241m=\u001B[39m combine_kwargs(engine_kwargs, kwargs)\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[43mif_sheet_exists\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mif_sheet_exists\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# the file and later write to it\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode:  \u001B[38;5;66;03m# Load from existing workbook\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1313\u001B[0m, in \u001B[0;36mExcelWriter.__init__\u001B[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1309\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handles \u001B[38;5;241m=\u001B[39m IOHandles(\n\u001B[0;32m   1310\u001B[0m     cast(IO[\u001B[38;5;28mbytes\u001B[39m], path), compression\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m}\n\u001B[0;32m   1311\u001B[0m )\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, ExcelWriter):\n\u001B[1;32m-> 1313\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1315\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1316\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cur_sheet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m date_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\io\\common.py:734\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    732\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[0;32m    733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[1;32m--> 734\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[0;32m    737\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    738\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\mapping_mistakes\\lib\\site-packages\\pandas\\io\\common.py:597\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    595\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[0;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: Cannot save file into a non-existent directory: 'H:\\analysis_mapping_mistakes\\US\\01-processed-data\\01-tweet_data\\inspect'"
     ]
    }
   ],
   "source": [
    "df.source.value_counts().to_excel(output_path + r\"\\inspect\\sources.xlsx\")\n",
    "df.source.value_counts().head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect tweets from instagram\n",
    "df.loc[df.source == \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\", \"tweet\"].head(500).to_excel(\n",
    "    output_path + r\"\\inspect\\instagram_examples.xlsx\")\n",
    "df[df.source == \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\"].head() # Does not look like bots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.source == \"\"\"<a href=\"https://www.careerarc.com\" rel=\"nofollow\">CareerArc 2.0</a>\"\"\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add column to identify tweets that are not posted through a third party API (???)\n",
    "keep_sources = [\"\"\"<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>\"\"\",\n",
    "                \"\"\"<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>\"\"\",\n",
    "                \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\",\n",
    "                \"\"\"<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>\"\"\"]\n",
    "\n",
    "df[\"nobot\"] = False\n",
    "df.loc[df[\"source\"].isin(keep_sources), \"nobot\"] = True\n",
    "df[df[\"nobot\"]].sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"nobot\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Inspect users with high number of statuses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby(\"nobot\")[\"user_statuses\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[(df[\"user_statuses\"] > 200000) & (df[\"nobot\"]),\n",
    "             [\"NAME\", \"tweet\", \"user_id\", \"user_description\", \"source\"]].drop_duplicates(subset=\"user_id\").head(10) # Does not look like bots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3. Exclude bots using source of tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only: iPhone, Android, Instagram, iPad\n",
    "df = df[df[\"nobot\"]]\n",
    "df.drop(\"nobot\", axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Compute local time for each tweet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect format (UCT time)\n",
    "df[\"created_at\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute time zone based on coordinates\n",
    "from tzwhere import tzwhere # Not sure why, but needs to be imported here\n",
    "tzwhere = tzwhere.tzwhere(forceTZ=True)\n",
    "df['lon'] = df.to_crs(\"epsg:4326\").best_coords.x\n",
    "df['lat'] = df.to_crs(\"epsg:4326\").best_coords.y\n",
    "df[\"timezone\"] = df.apply(lambda x: tzwhere.tzNameAt(x[\"lat\"], x[\"lon\"], forceTZ=True), axis = 1)\n",
    "print(sum(df[\"timezone\"].isna())) # Should be none\n",
    "df[\"timezone\"].value_counts().head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute local time for each tweet\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format = '%a %b %d %H:%M:%S +0000 %Y', utc=True)\n",
    "def tz_func(x):\n",
    "    return x.dt.tz_convert(x.name).dt.tz_localize(tz=None)\n",
    "df[\"local_time\"] = df.groupby(\"timezone\")[\"created_at\"].transform(tz_func)\n",
    "df[[\"created_at\", \"timezone\", \"local_time\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create separate variables for day and time\n",
    "df[\"hour\"] = df[\"local_time\"].dt.hour\n",
    "df[\"day\"] = df[\"local_time\"].dt.weekday # 0 = Monday, 6 = Sunday\n",
    "print(df[\"hour\"].value_counts())\n",
    "print(df[\"day\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define weekdays\n",
    "df[\"weekday\"] = df[\"day\"] < 5\n",
    "df[\"weekday\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distinguish between free time and worktime\n",
    "df[\"workhours\"] = (df[\"weekday\"]) & (df[\"hour\"] >= 9) & (df[\"hour\"] < 17)\n",
    "df[\"workhours\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Handle users that tweet from different counties"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect nr of counties users tweet from\n",
    "counties_per_user = df.groupby([\"user_id\", \"GEOID\"]).count()\n",
    "counties_per_user = counties_per_user.groupby(\"user_id\")[\"lon\"].count().sort_values(ascending=False)\n",
    "print(counties_per_user.describe())\n",
    "print(len(counties_per_user[counties_per_user>5]))\n",
    "print(len(counties_per_user[counties_per_user>10]))\n",
    "print(len(counties_per_user[counties_per_user>20]))\n",
    "print(len(counties_per_user[counties_per_user>50]))\n",
    "print(len(counties_per_user[counties_per_user>100]))\n",
    "print(len(counties_per_user[counties_per_user==1]))\n",
    "counties_per_user.value_counts(normalize=True).head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect users who tweet from many counties\n",
    "df[\"nr_counties\"] = df.groupby(\"user_id\")[\"GEOID\"].transform(\"nunique\") # add variable for nr or counties each user tweets from\n",
    "if mode == \"full\":\n",
    "    df.loc[df[\"nr_counties\"] > 50, [\"NAME\", \"tweet\", \"user_description\"]].sample(20)  # Looks like normal users (e.g. truck drivers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get main county/counties for each user\n",
    "df[\"main_county_all\"] = df.groupby(\"user_id\")[\"GEOID\"].transform(lambda x: \",\".join(st.multimode(x)))\n",
    "df[\"nr_main_counties\"] = df.groupby(\"user_id\")[\"GEOID\"].transform(lambda x: len(st.multimode(x)))\n",
    "print(df[\"main_county_all\"].value_counts().head())\n",
    "print(df[\"nr_main_counties\"].value_counts())\n",
    "print(df[\"nr_main_counties\"].value_counts(normalize=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get main county/counties for each user excluding tweets during workhours or weekends (when people might not be at home)\n",
    "df[\"county_home\"] = df[\"GEOID\"]*1\n",
    "df[\"county_home\"].loc[(df[\"weekday\"] == False) | (df[\"workhours\"] == True)] = np.nan\n",
    "df[\"main_county_home\"] = df.groupby(\"user_id\")[\"county_home\"].transform(lambda x: \",\".join(\n",
    "    st.multimode(x[x.notna()])))\n",
    "\n",
    "df[\"nr_home_counties\"] = df.groupby(\"user_id\")[\"county_home\"].transform(lambda x: len(st.multimode(x[x.notna()])))\n",
    "print(df[\"main_county_home\"].value_counts().head())\n",
    "print(df[\"nr_home_counties\"].value_counts())\n",
    "print(df[\"nr_home_counties\"].value_counts(normalize=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[\"main_county_all\", \"main_county_home\", \"nr_counties\", \"nr_home_counties\"]].head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define var \"main_county\": main county if unambiguous, else main home county (based on time) if unambiguous, else NaN\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"main_county\"] = df.apply(lambda x:\n",
    "                                     x[\"main_county_all\"] if x[\"nr_main_counties\"] == 1\n",
    "                                     else (x[\"main_county_home\"] if x[\"nr_home_counties\"] == 1\n",
    "                                           else np.nan),\n",
    "                                     axis=1)\n",
    "df[\"main_county\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect number of ambiguous cases\n",
    "print(df[\"main_county\"].isna().sum())\n",
    "print(df[\"main_county\"].isna().mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[\"GEOID\", \"user_id\", \"main_county_all\", \"main_county\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get county-user for main county\n",
    "print(\"Total nr of counties: \", len(counties[\"GEOID\"].unique()))\n",
    "print(\"Nr of counties with at least one tweet: \", len(df[\"GEOID\"].unique()))\n",
    "print(\"Nr of counties with at least one tweet as main county: \", len(df[\"main_county\"].unique()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby(\"main_county\")[\"user_id\"].count().describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Finalize and export dataset and ID file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1. Clean and add columns with additional information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop redundant columns\n",
    "df.drop(['main_county_all',\n",
    "         'nr_main_counties',\n",
    "         'county_home',\n",
    "         'main_county_home',\n",
    "         'nr_home_counties'],\n",
    "        inplace = True,\n",
    "        axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Place type and coordinates\n",
    "df.drop(['best_coords'],\n",
    "        inplace = True, axis = 1)\n",
    "df[\"place_type\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# County name and ID\n",
    "df.rename({\"GEOID\":\"county_id_tweet\",\n",
    "           \"NAME\" :\"county_name_tweet\",\n",
    "           \"main_county\": \"county_id_user\"},\n",
    "          axis=1, inplace=True)\n",
    "df = df.merge(counties[[\"GEOID\", \"NAME\"]],\n",
    "              left_on=\"county_id_user\",\n",
    "              right_on=\"GEOID\",\n",
    "              how=\"inner\")\n",
    "df.drop(\"GEOID\", inplace=True, axis=1)\n",
    "df.rename({\"NAME\": \"county_name_user\"}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# iPhone users\n",
    "df[\"iphone\"] = df[\"source\"].isin([\"\"\"<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>\"\"\",\n",
    "                                  \"\"\"<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>\"\"\"])\n",
    "df[\"iphone\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instagram retweets\n",
    "df[\"insta\"] = df[\"source\"] == \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\"\n",
    "print(df[\"insta\"].value_counts(normalize=True))\n",
    "\n",
    "# Check Instagram tweets with default retweet text\n",
    "insta = df[df[\"source\"] == \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\"]\n",
    "print(insta[\"tweet\"].str.startswith(\"Just posted a photo\").value_counts(normalize=True))\n",
    "\n",
    "# Exclude Instagram tweets with default retweet text\n",
    "df = df.loc[~((df[\"source\"] == \"\"\"<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>\"\"\") &\n",
    "              (df[\"tweet\"].str.startswith(\"Just posted a photo\"))), :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Length of tweet and user description\n",
    "df[\"tweet_length\"] = df[\"tweet\"].str.len()\n",
    "df[\"user_length\"] = df[\"user_description\"].str.len()\n",
    "df.loc[df[\"user_length\"].isna(), \"user_length\"] = 0\n",
    "print(df[\"tweet_length\"].describe())\n",
    "print(df[\"user_length\"].describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mobility of users\n",
    "df.rename({\"nr_counties\":\"user_nr_counties\"}, inplace=True, axis=1)\n",
    "df[\"user_nr_counties\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Order columns\n",
    "move_cols = ['county_id_tweet', 'county_name_tweet', 'county_id_user', 'county_name_user']\n",
    "cols  = move_cols + [col for col in df.columns if col not in move_cols]\n",
    "df = df[cols]\n",
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop tweets in US territories (Puerto Rico, Virgin Islands)\n",
    "df[\"state_id_user\"] = df[\"county_id_user\"].str[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[df[\"state_id_user\"].astype(int) >=60, \"state_id_user\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df[df[\"state_id_user\"].astype(int)<60] # Drop if not in a US state (but only in territories)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df[\"state_id_user\"].value_counts())\n",
    "print(len(df[\"state_id_user\"].value_counts()))\n",
    "df.drop(columns=\"state_id_user\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2. Export files\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export ID file\n",
    "df[[\"id\", \"user_id\", \"county_id_tweet\", \"county_id_user\"]].to_csv(output_path + r\"\\ids.csv\",\n",
    "                                                                  encoding=\"utf-8\",\n",
    "                                                                  sep=\";\",\n",
    "                                                                  index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export full dataset (takes long)\n",
    "df.to_csv(output_path + r\"\\tweet_data.csv\",\n",
    "          encoding=\"utf-8\",\n",
    "          sep=\";\",\n",
    "          index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print nr of users and tweets in final dataset\n",
    "print(len(df))\n",
    "print(df[\"user_id\"].nunique())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"place_type\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
